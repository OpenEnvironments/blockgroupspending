{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9be4d68f-58bd-4720-a721-e60ec44493a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "\n",
    "This process trains regression models of consumer expenditure based on \n",
    "demographic properties from the BLS Consumer Expenditure Survey.\n",
    "\n",
    "    Independent variables are calculated including bins, indicators to help tree based methods\n",
    "    A model is created for each \n",
    "        race 'white','black','asian','other','hispanic'\n",
    "        and subset of spending 'TOTEXP','FOOD','HOUS','TRANS','HEALTH','RETPEN'\n",
    "    During prediction an additional property is calculated as the difference between the total and subsets 'OTHER'\n",
    "    Finally, the list of models is saved to a pickle file for the next process, penetration.\n",
    "    \n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dependents = ['TOTEXP','FOOD','ALCBEV','HOUS','APPAR','TRANS','HEALTH', 'ENTERT',\n",
    "              'PERSCA', 'READ','EDUCA','TOBACC','MISC', 'CASHCO','RETPEN']\n",
    "\n",
    "independents = ['income_z',\n",
    "                'income_A', 'income_B', 'income_C', 'income_D', 'income_E', 'income_F', 'income_G', \n",
    "                'homeownership',\n",
    "                'vehicles_0_1',\n",
    "                'vehicles_0', 'vehicles_1', 'vehicles_2', 'vehicles_3', 'vehicles_4', 'vehicles_5plus',\n",
    "                'married',\n",
    "                'children',\n",
    "                'bedroomstwofewer',\n",
    "                'bedrooms_0', 'bedrooms_1', 'bedrooms_2', 'bedrooms_3', 'bedrooms_4', 'bedrooms_5plus',\n",
    "                'fuel_log',\n",
    "                'fuel_A', 'fuel_B', 'fuel_C', 'fuel_D', 'fuel_E', 'fuel_F', 'fuel_G',\n",
    "                'familysize_1','familysize_2','familysize_3','familysize_4','familysize_5','familysize_6','familysize_7more',\n",
    "                #'income_low','income_lowermid','income_uppermid','income_high',\n",
    "               ]\n",
    "\n",
    "\n",
    "#\n",
    "# Prepare the FMLI for training, adding derived variable, addressing NAs etc.\n",
    "#\n",
    "\n",
    "fmli = pd.read_pickle('fmli.pkl')\n",
    "\n",
    "# Incomplete interviews\n",
    "fmli = fmli[fmli.TOTEXP > 0].copy()            # 1627 rows removed, incomplete surveys\n",
    "fmli = fmli[fmli.FINCBTXM > 0].copy()          # 70 rows removed\n",
    "fmli = fmli[~fmli.BEDROOMQ.isna()].copy()      # 68 rows have NA\n",
    "fmli[fmli.HEALTH>=0].copy()                    # 44 rows are negative\n",
    "\n",
    "fmli = fmli[(fmli['NTLGASCQ'] + fmli['NTLGASPQ'] + # 1654 rows have no fuel spending\n",
    "             fmli['ELCTRCCQ'] + fmli['ELCTRCPQ'] + fmli['ALLFULCQ'] + fmli['ALLFULPQ'])>0]\n",
    "\n",
    "# Enforce integers\n",
    "for var in ['REF_RACE','HISP_REF','TOTEXP','FINCBTXM','CUTENURE','BEDROOMQ','VEHQ','FAM_TYPE',\n",
    "            'NTLGASCQ','NTLGASPQ','ELCTRCCQ','ELCTRCPQ','ALLFULCQ','ALLFULPQ',]:\n",
    "    fmli[var] = fmli[var].astype(int)\n",
    "\n",
    "# Creating a model for each race, with hispanic like another race\n",
    "fmli['model_white'] = np.where((fmli.REF_RACE == 1) & (fmli.HISP_REF == 2),1,0)\n",
    "fmli['model_black'] = np.where((fmli.REF_RACE == 2) & (fmli.HISP_REF == 2),1,0)\n",
    "fmli['model_asian'] = np.where((fmli.REF_RACE == 4) & (fmli.HISP_REF == 2),1,0)\n",
    "fmli['model_other'] = np.where((~fmli.REF_RACE.isin([1,2,4])) & (fmli.HISP_REF == 2),1,0)\n",
    "fmli['model_hispanic'] = np.where(fmli.HISP_REF == 1,1,0)\n",
    "\n",
    "# Calculate the dependent variables\n",
    "fmli['income_log'] = np.log(fmli.FINCBTXM)\n",
    "fmli['income_z'] = (fmli.income_log-fmli.income_log.mean())/fmli.income_log.std()\n",
    "fmli['homeownership'] = (fmli.CUTENURE.astype(int) < 4).astype(int) \n",
    "fmli['vehicles_0_1'] = (fmli.VEHQ < 2).astype(int)\n",
    "fmli['vehicles_0'] = np.where(fmli.VEHQ == 0,1,0)\n",
    "fmli['vehicles_1'] = np.where(fmli.VEHQ == 1,1,0)\n",
    "fmli['vehicles_2'] = np.where(fmli.VEHQ == 2,1,0)\n",
    "fmli['vehicles_3'] = np.where(fmli.VEHQ == 3,1,0)\n",
    "fmli['vehicles_4'] = np.where(fmli.VEHQ == 4,1,0)\n",
    "fmli['vehicles_5plus'] = np.where(fmli.VEHQ >= 5,1,0)\n",
    "fmli[\"married\"] = (fmli.FAM_TYPE < 6).astype(int)\n",
    "fmli[\"children\"] = (fmli.FAM_TYPE.isin([2,3,4,6,7])).astype(int)\n",
    "fmli['bedroomstwofewer'] = np.where(fmli.BEDROOMQ<3,1,0)\n",
    "fmli['bedrooms_0'] = np.where(fmli.BEDROOMQ == 0,1,0)\n",
    "fmli['bedrooms_1'] = np.where(fmli.BEDROOMQ == 1,1,0)\n",
    "fmli['bedrooms_2'] = np.where(fmli.BEDROOMQ == 2,1,0)\n",
    "fmli['bedrooms_3'] = np.where(fmli.BEDROOMQ == 3,1,0)\n",
    "fmli['bedrooms_4'] = np.where(fmli.BEDROOMQ == 4,1,0)\n",
    "fmli['bedrooms_5plus'] = np.where(fmli.BEDROOMQ >= 5,1,0)\n",
    "fmli['fuel_log'] = np.log(1+fmli['NTLGASCQ']+fmli['NTLGASPQ']+fmli['ELCTRCCQ'] + fmli['ELCTRCPQ']+fmli['ALLFULCQ']+fmli['ALLFULPQ'])\n",
    "fmli['fuel_z'] = (fmli['fuel_log']-fmli['fuel_log'].mean()) / fmli['fuel_log'].std()\n",
    "fmli['familysize_1'] = np.where(fmli.FAM_SIZE == 1, 1, 0)\n",
    "fmli['familysize_2'] = np.where(fmli.FAM_SIZE == 2, 1, 0)\n",
    "fmli['familysize_3'] = np.where(fmli.FAM_SIZE == 3, 1, 0)\n",
    "fmli['familysize_4'] = np.where(fmli.FAM_SIZE == 4, 1, 0)\n",
    "fmli['familysize_5'] = np.where(fmli.FAM_SIZE == 5, 1, 0)\n",
    "fmli['familysize_6'] = np.where(fmli.FAM_SIZE == 6, 1, 0)\n",
    "fmli['familysize_7more'] = np.where(fmli.FAM_SIZE >= 7, 1, 0)\n",
    "# Using income brackets did not improve predictiveness\n",
    "#fmli[\"income_low\"]      = np.where(fmli.FINCBTXM <  35000,1,0)\n",
    "#fmli[\"income_lowermid\"] = np.where((fmli.FINCBTXM >= 35000)&(fmli.FINCBTXM <  75000),1,0)\n",
    "#fmli[\"income_uppermid\"] = np.where((fmli.FINCBTXM >= 75000)&(fmli.FINCBTXM < 130000),1,0)\n",
    "#fmli[\"income_high\"]     = np.where(fmli.FINCBTXM >= 130000,1,0)\n",
    "fmli = pd.concat( \\\n",
    "    [fmli.reset_index(drop=True),\n",
    "     pd.get_dummies(\\\n",
    "                     np.where(fmli['income_z'] < -1.5, 'A',\n",
    "                     np.where(fmli['income_z'] < -.75, 'B',\n",
    "                     np.where(fmli['income_z'] < -.25, 'C',\n",
    "                     np.where(fmli['income_z'] < .25, 'D',\n",
    "                     np.where(fmli['income_z'] < .75, 'E', \n",
    "                     np.where(fmli['income_z'] < 1.5, 'F', 'G')))))),prefix=\"income\", dtype=int).reset_index(drop=True),\n",
    "    pd.get_dummies(\\\n",
    "                     np.where(fmli['fuel_z'] < -1.5, 'A',\n",
    "                     np.where(fmli['fuel_z'] < -.75, 'B',\n",
    "                     np.where(fmli['fuel_z'] < -.25, 'C',\n",
    "                     np.where(fmli['fuel_z'] < .25, 'D',\n",
    "                     np.where(fmli['fuel_z'] < .75, 'E', \n",
    "                     np.where(fmli['fuel_z'] < 1.5, 'F', 'G')))))),prefix=\"fuel\", dtype=int).reset_index(drop=True),\n",
    "    ], axis=1)\n",
    "\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1d08a6c-f7c3-4023-a679-69634b67585b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         4464.0\n",
       "1         1215.0\n",
       "2         4677.0\n",
       "3         3100.0\n",
       "4         6742.0\n",
       "          ...   \n",
       "21885     3666.0\n",
       "21886     2173.5\n",
       "21887    16146.0\n",
       "21888    10099.0\n",
       "21889    11134.0\n",
       "Name: HOUS, Length: 21890, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmli['HOUS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db692a6e-b2d1-4d53-863e-55d7ae115b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FINLWT21',\n",
       " 'WTREP01',\n",
       " 'WTREP02',\n",
       " 'WTREP03',\n",
       " 'WTREP04',\n",
       " 'WTREP05',\n",
       " 'WTREP06',\n",
       " 'WTREP07',\n",
       " 'WTREP08',\n",
       " 'WTREP09',\n",
       " 'WTREP10',\n",
       " 'WTREP11',\n",
       " 'WTREP12',\n",
       " 'WTREP13',\n",
       " 'WTREP14',\n",
       " 'WTREP15',\n",
       " 'WTREP16',\n",
       " 'WTREP17',\n",
       " 'WTREP18',\n",
       " 'WTREP19',\n",
       " 'WTREP20',\n",
       " 'WTREP21',\n",
       " 'WTREP22',\n",
       " 'WTREP23',\n",
       " 'WTREP24',\n",
       " 'WTREP25',\n",
       " 'WTREP26',\n",
       " 'WTREP27',\n",
       " 'WTREP28',\n",
       " 'WTREP29',\n",
       " 'WTREP30',\n",
       " 'WTREP31',\n",
       " 'WTREP32',\n",
       " 'WTREP33',\n",
       " 'WTREP34',\n",
       " 'WTREP35',\n",
       " 'WTREP36',\n",
       " 'WTREP37',\n",
       " 'WTREP38',\n",
       " 'WTREP39',\n",
       " 'WTREP40',\n",
       " 'WTREP41',\n",
       " 'WTREP42',\n",
       " 'WTREP43',\n",
       " 'WTREP44',\n",
       " 'WTD_TOTEXPCQ',\n",
       " 'WTD_FOODCQ',\n",
       " 'WTD_ALCBEVCQ',\n",
       " 'WTD_HOUSCQ',\n",
       " 'WTD_APPARCQ',\n",
       " 'WTD_TRANSCQ',\n",
       " 'WTD_HEALTHCQ',\n",
       " 'WTD_ENTERTCQ',\n",
       " 'WTD_PERSCACQ',\n",
       " 'WTD_READCQ',\n",
       " 'WTD_EDUCACQ',\n",
       " 'WTD_TOBACCCQ',\n",
       " 'WTD_LIFINSCQ',\n",
       " 'WTD_MISCCQ',\n",
       " 'WTD_CASHCOCQ',\n",
       " 'WTD_RETPENCQ',\n",
       " 'WTD_TOTEXPPQ',\n",
       " 'WTD_FOODPQ',\n",
       " 'WTD_ALCBEVPQ',\n",
       " 'WTD_HOUSPQ',\n",
       " 'WTD_APPARPQ',\n",
       " 'WTD_TRANSPQ',\n",
       " 'WTD_HEALTHPQ',\n",
       " 'WTD_ENTERTPQ',\n",
       " 'WTD_PERSCAPQ',\n",
       " 'WTD_READPQ',\n",
       " 'WTD_EDUCAPQ',\n",
       " 'WTD_TOBACCPQ',\n",
       " 'WTD_LIFINSPQ',\n",
       " 'WTD_MISCPQ',\n",
       " 'WTD_CASHCOPQ',\n",
       " 'WTD_RETPENPQ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c for c in fmli if \"WT\" in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebecac0-b9ce-47e3-81db-b9d159ddf460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74cb9d2-b4cd-4a9d-b54f-9975eadda755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1a01c2e-3240-41fc-865e-33125a88ed91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting WTD_TOTEXP for white\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "Best score:  0.13858642578125\n",
      "    0.14162856340408325\n",
      "Duration: 444 seconds\n",
      "Fitting WTD_TOTEXP for black\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "Best score:  0.08386697769165039\n",
      "    0.11693763732910156\n",
      "Duration: 311 seconds\n",
      "Fitting WTD_TOTEXP for asian\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.9, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.7}\n",
      "Best score:  0.16426796913146974\n",
      "    0.1956067681312561\n",
      "Duration: 305 seconds\n",
      "Fitting WTD_TOTEXP for other\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.2}\n",
      "Best score:  -0.0004641890525817871\n",
      "    0.0028766393661499023\n",
      "Duration: 293 seconds\n",
      "Fitting WTD_TOTEXP for hispanic\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "Best score:  0.21948091983795165\n",
      "    0.08838158845901489\n",
      "Duration: 333 seconds\n",
      "Fitting WTD_FOOD for white\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.4}\n",
      "Best score:  0.25948942223383853\n",
      "    0.19348058957231207\n",
      "Duration: 359 seconds\n",
      "Fitting WTD_FOOD for black\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.7}\n",
      "Best score:  0.29541662222763554\n",
      "    0.26042684928978055\n",
      "Duration: 286 seconds\n",
      "Fitting WTD_FOOD for asian\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "Best score:  0.14575130526345811\n",
      "    0.07936994039688616\n",
      "Duration: 279 seconds\n",
      "Fitting WTD_FOOD for other\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.2}\n",
      "Best score:  -0.06716308377005324\n",
      "    0.030837063431821465\n",
      "Duration: 279 seconds\n",
      "Fitting WTD_FOOD for hispanic\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.4}\n",
      "Best score:  0.20433537407259034\n",
      "    0.17355598741105627\n",
      "Duration: 282 seconds\n",
      "Fitting WTD_HOUS for white\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "Best score:  0.22201547763280574\n",
      "    0.2342264017389596\n",
      "Duration: 303 seconds\n",
      "Fitting WTD_HOUS for black\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.2}\n",
      "Best score:  0.2931602312152247\n",
      "    0.21671228494257333\n",
      "Duration: 282 seconds\n",
      "Fitting WTD_HOUS for asian\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.7}\n",
      "Best score:  0.26663658857259803\n",
      "    0.1942694987104001\n",
      "Duration: 263 seconds\n",
      "Fitting WTD_HOUS for other\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "Best score:  -0.1110605212476026\n",
      "    0.1276288512696131\n",
      "Duration: 273 seconds\n",
      "Fitting WTD_HOUS for hispanic\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "Best score:  0.1342455129744824\n",
      "    0.1414240296882594\n",
      "Duration: 288 seconds\n",
      "Fitting WTD_TRANS for white\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "Best score:  0.03721136935533158\n",
      "    0.03546387313226773\n",
      "Duration: 320 seconds\n",
      "Fitting WTD_TRANS for black\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "Best score:  -0.07907455716548292\n",
      "    0.029416687308030598\n",
      "Duration: 293 seconds\n",
      "Fitting WTD_TRANS for asian\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.9, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "Best score:  -0.04525159975342234\n",
      "    -0.014694251674783443\n",
      "Duration: 294 seconds\n",
      "Fitting WTD_TRANS for other\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.2}\n",
      "Best score:  -0.4879470029674914\n",
      "    0.030336239451524416\n",
      "Duration: 295 seconds\n",
      "Fitting WTD_TRANS for hispanic\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "Best score:  -0.03462295541361453\n",
      "    0.007511686064112677\n",
      "Duration: 296 seconds\n",
      "Fitting WTD_HEALTH for white\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "Best score:  0.07272555243469173\n",
      "    0.019026461017914387\n",
      "Duration: 322 seconds\n",
      "Fitting WTD_HEALTH for black\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.4}\n",
      "Best score:  0.3455272095381899\n",
      "    0.13703357183611364\n",
      "Duration: 294 seconds\n",
      "Fitting WTD_HEALTH for asian\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "Best score:  0.2874239293098598\n",
      "    0.09988407509613373\n",
      "Duration: 295 seconds\n",
      "Fitting WTD_HEALTH for other\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "Best score:  -0.1647813072595411\n",
      "    -0.002832957055494578\n",
      "Duration: 295 seconds\n",
      "Fitting WTD_HEALTH for hispanic\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.4}\n",
      "Best score:  0.22159610135354285\n",
      "    0.12625969854456665\n",
      "Duration: 296 seconds\n",
      "Fitting WTD_RETPEN for white\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "Best score:  0.5118520215285949\n",
      "    0.4018453089220867\n",
      "Duration: 321 seconds\n",
      "Fitting WTD_RETPEN for black\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.9, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "Best score:  0.6279695535714568\n",
      "    0.5323062972876416\n",
      "Duration: 294 seconds\n",
      "Fitting WTD_RETPEN for asian\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "Best score:  0.49075268519938015\n",
      "    0.46671434481131524\n",
      "Duration: 295 seconds\n",
      "Fitting WTD_RETPEN for other\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "Best score:  0.35991207843825207\n",
      "    0.3137130897875485\n",
      "Duration: 294 seconds\n",
      "Fitting WTD_RETPEN for hispanic\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'random_state': 1234, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.2}\n",
      "Best score:  0.6083425452496092\n",
      "    0.4445412515605346\n",
      "Duration: 294 seconds\n",
      "Models saved\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Regression models\n",
    "#\n",
    "\n",
    "\n",
    "# The models will be kept in a dictionary of dictionaries\n",
    "races = ['white','black','asian','other','hispanic',]\n",
    "targets = ['TOTEXP','FOOD','HOUS','TRANS','HEALTH','RETPEN',]\n",
    "\n",
    "for target in targets:\n",
    "    fmli[\"WTD_\"+target] = fmli[target] * fmli['FINLWT21']\n",
    "\n",
    "models = {}\n",
    "for target in [\"WTD_\"+t for t in targets]:\n",
    "    targetmodels = {}\n",
    "    for race in races:\n",
    "        starting = datetime.now()\n",
    "        print(\"Fitting\",target, \"for\", race)\n",
    "        X = fmli[independents][fmli[\"model_\"+race] == 1]\n",
    "        y = fmli[target][fmli[\"model_\"+race] == 1]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test)        \n",
    "        model = xgb.XGBRegressor()\n",
    "        param_grid = {\n",
    "            'objective': ['reg:squarederror'],  \n",
    "            'max_depth': [3, 5], # [1, 3, 7],\n",
    "            'learning_rate': [0.01],  # also called eta.  Tried 0.75, 0.5, 0.25, 0.1, \n",
    "            'subsample': [0.2, 0.4, 0.7, 0.9], #, 0.2, 0.3, 0.4, 0.5, 0.7\n",
    "            'colsample_bytree':  [0.5, 0.7, 0.9],  # 0.4,  0.6, 0.8\n",
    "            'reg_alpha':  [0.5], # L1 regularization [0, 0.1, 0.5, 1, 5, 10, ...\n",
    "            'reg_lambda': [0.5], # L2 regularization [0, 0.1, 0.5, 1, 5, 10, ...\n",
    "            'n_estimators': [1000], # 100, 250, \n",
    "            'eval_metric':['rmse'],\n",
    "            'random_state': [1234],\n",
    "        }\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5)    \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        print(\"Best set of hyperparameters: \", grid_search.best_params_)\n",
    "        print(\"Best score: \", grid_search.best_score_)\n",
    "        model = xgb.train(grid_search.best_params_, dtrain, num_boost_round=100)\n",
    "        y_pred = model.predict(dtest)\n",
    "        print(\"   \",r2_score(y_test, y_pred))\n",
    "        targetmodels[race] = model\n",
    "        duration = datetime.now() - starting\n",
    "        print(f\"Duration: {duration.seconds} seconds\")\n",
    "    models[target] = targetmodels\n",
    "\n",
    "with open(\"blockgroup_regression_models.pkl\", 'wb') as file:  \n",
    "    pickle.dump(models, file)\n",
    "\n",
    "print(\"Models saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a75a29-c85c-4d8d-836c-59693576e3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee91cb50-0a7b-4eaa-b675-3cbcc822f2fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b860ad2d-cbe4-498d-85ac-02d620b65a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b8f06-4b96-4d26-bb23-dd748e303069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4c56d3-1369-45ce-92ce-1555d64cf800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805549aa-2edf-4215-8e8c-768572c22bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd578a-56ef-4a01-945b-1c2da145d56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f85138-ce85-4ce4-a290-2fe1a36477da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d9b28-11ab-4f01-bada-edfeea79ce89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5075a12-4abe-4339-aa10-bced9dfe0bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93bebba-8dbf-40d6-8326-b17cedba1aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb69f8bc-9b08-412a-84a6-64aeefb1c90f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ccea8-7b16-4e15-8f9a-9ce67a391ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee92978-f3c6-4281-aad9-b4208e9c8bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blockgroupspending",
   "language": "python",
   "name": "blockgroupspending"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be4d68f-58bd-4720-a721-e60ec44493a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "\n",
    "This process trains regression models of consumer expenditure based on \n",
    "demographic properties from the BLS Consumer Expenditure Survey.\n",
    "\n",
    "    Independent variables are calculated including bins, indicators to help tree based methods\n",
    "    A model is created for each \n",
    "        race 'white','black','asian','other','hispanic'\n",
    "        and subset of spending 'TOTEXP','FOOD','HOUS','TRANS','HEALTH','RETPEN'\n",
    "    During prediction an additional property is calculated as the difference between the total and subsets 'OTHER'\n",
    "    Finally, the list of models is saved to a pickle file for the next process, penetration.\n",
    "    \n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dependents = ['TOTEXP','FOOD','ALCBEV','HOUS','APPAR','TRANS','HEALTH', 'ENTERT',\n",
    "              'PERSCA', 'READ','EDUCA','TOBACC','MISC', 'CASHCO','RETPEN']\n",
    "\n",
    "independents = ['income_z',\n",
    "                'income_A', 'income_B', 'income_C', 'income_D', 'income_E', 'income_F', 'income_G', \n",
    "                'homeownership',\n",
    "                'vehicles_0_1',\n",
    "                'vehicles_0', 'vehicles_1', 'vehicles_2', 'vehicles_3', 'vehicles_4', 'vehicles_5plus',\n",
    "                'married',\n",
    "                'children',\n",
    "                'bedroomstwofewer',\n",
    "                'bedrooms_0', 'bedrooms_1', 'bedrooms_2', 'bedrooms_3', 'bedrooms_4', 'bedrooms_5plus',\n",
    "                'fuel_log',\n",
    "                'fuel_A', 'fuel_B', 'fuel_C', 'fuel_D', 'fuel_E', 'fuel_F', 'fuel_G',\n",
    "                'familysize_1','familysize_2','familysize_3','familysize_4','familysize_5','familysize_6','familysize_7more',\n",
    "                #'income_low','income_lowermid','income_uppermid','income_high',\n",
    "               ]\n",
    "\n",
    "\n",
    "#\n",
    "# Prepare the FMLI for training, adding derived variable, addressing NAs etc.\n",
    "#\n",
    "\n",
    "fmli = pd.read_pickle('fmli.pkl')\n",
    "\n",
    "# Incomplete interviews\n",
    "fmli = fmli[fmli.TOTEXP > 0].copy()            # 1627 rows removed, incomplete surveys\n",
    "fmli = fmli[fmli.FINCBTXM > 0].copy()          # 70 rows removed\n",
    "fmli = fmli[~fmli.BEDROOMQ.isna()].copy()      # 68 rows have NA\n",
    "fmli[fmli.HEALTH>=0].copy()                    # 44 rows are negative\n",
    "\n",
    "fmli = fmli[(fmli['NTLGASCQ'] + fmli['NTLGASPQ'] + # 1654 rows have no fuel spending\n",
    "             fmli['ELCTRCCQ'] + fmli['ELCTRCPQ'] + fmli['ALLFULCQ'] + fmli['ALLFULPQ'])>0]\n",
    "\n",
    "# Enforce integers\n",
    "for var in ['REF_RACE','HISP_REF','TOTEXP','FINCBTXM','CUTENURE','BEDROOMQ','VEHQ','FAM_TYPE',\n",
    "            'NTLGASCQ','NTLGASPQ','ELCTRCCQ','ELCTRCPQ','ALLFULCQ','ALLFULPQ',]:\n",
    "    fmli[var] = fmli[var].astype(int)\n",
    "\n",
    "# Creating a model for each race, with hispanic like another race\n",
    "fmli['model_white'] = np.where((fmli.REF_RACE == 1) & (fmli.HISP_REF == 2),1,0)\n",
    "fmli['model_black'] = np.where((fmli.REF_RACE == 2) & (fmli.HISP_REF == 2),1,0)\n",
    "fmli['model_asian'] = np.where((fmli.REF_RACE == 4) & (fmli.HISP_REF == 2),1,0)\n",
    "fmli['model_other'] = np.where((~fmli.REF_RACE.isin([1,2,4])) & (fmli.HISP_REF == 2),1,0)\n",
    "fmli['model_hispanic'] = np.where(fmli.HISP_REF == 1,1,0)\n",
    "\n",
    "# Calculate the dependent variables\n",
    "fmli['income_log'] = np.log(fmli.FINCBTXM)\n",
    "fmli['income_z'] = (fmli.income_log-fmli.income_log.mean())/fmli.income_log.std()\n",
    "fmli['homeownership'] = (fmli.CUTENURE.astype(int) < 4).astype(int) \n",
    "fmli['vehicles_0_1'] = (fmli.VEHQ < 2).astype(int)\n",
    "fmli['vehicles_0'] = np.where(fmli.VEHQ == 0,1,0)\n",
    "fmli['vehicles_1'] = np.where(fmli.VEHQ == 1,1,0)\n",
    "fmli['vehicles_2'] = np.where(fmli.VEHQ == 2,1,0)\n",
    "fmli['vehicles_3'] = np.where(fmli.VEHQ == 3,1,0)\n",
    "fmli['vehicles_4'] = np.where(fmli.VEHQ == 4,1,0)\n",
    "fmli['vehicles_5plus'] = np.where(fmli.VEHQ >= 5,1,0)\n",
    "fmli[\"married\"] = (fmli.FAM_TYPE < 6).astype(int)\n",
    "fmli[\"children\"] = (fmli.FAM_TYPE.isin([2,3,4,6,7])).astype(int)\n",
    "fmli['bedroomstwofewer'] = np.where(fmli.BEDROOMQ<3,1,0)\n",
    "fmli['bedrooms_0'] = np.where(fmli.BEDROOMQ == 0,1,0)\n",
    "fmli['bedrooms_1'] = np.where(fmli.BEDROOMQ == 1,1,0)\n",
    "fmli['bedrooms_2'] = np.where(fmli.BEDROOMQ == 2,1,0)\n",
    "fmli['bedrooms_3'] = np.where(fmli.BEDROOMQ == 3,1,0)\n",
    "fmli['bedrooms_4'] = np.where(fmli.BEDROOMQ == 4,1,0)\n",
    "fmli['bedrooms_5plus'] = np.where(fmli.BEDROOMQ >= 5,1,0)\n",
    "fmli['fuel_log'] = np.log(1+fmli['NTLGASCQ']+fmli['NTLGASPQ']+fmli['ELCTRCCQ'] + fmli['ELCTRCPQ']+fmli['ALLFULCQ']+fmli['ALLFULPQ'])\n",
    "fmli['fuel_z'] = (fmli['fuel_log']-fmli['fuel_log'].mean()) / fmli['fuel_log'].std()\n",
    "fmli['familysize_1'] = np.where(fmli.FAM_SIZE == 1, 1, 0)\n",
    "fmli['familysize_2'] = np.where(fmli.FAM_SIZE == 2, 1, 0)\n",
    "fmli['familysize_3'] = np.where(fmli.FAM_SIZE == 3, 1, 0)\n",
    "fmli['familysize_4'] = np.where(fmli.FAM_SIZE == 4, 1, 0)\n",
    "fmli['familysize_5'] = np.where(fmli.FAM_SIZE == 5, 1, 0)\n",
    "fmli['familysize_6'] = np.where(fmli.FAM_SIZE == 6, 1, 0)\n",
    "fmli['familysize_7more'] = np.where(fmli.FAM_SIZE >= 7, 1, 0)\n",
    "# Using income brackets did not improve predictiveness\n",
    "#fmli[\"income_low\"]      = np.where(fmli.FINCBTXM <  35000,1,0)\n",
    "#fmli[\"income_lowermid\"] = np.where((fmli.FINCBTXM >= 35000)&(fmli.FINCBTXM <  75000),1,0)\n",
    "#fmli[\"income_uppermid\"] = np.where((fmli.FINCBTXM >= 75000)&(fmli.FINCBTXM < 130000),1,0)\n",
    "#fmli[\"income_high\"]     = np.where(fmli.FINCBTXM >= 130000,1,0)\n",
    "fmli = pd.concat( \\\n",
    "    [fmli.reset_index(drop=True),\n",
    "     pd.get_dummies(\\\n",
    "                     np.where(fmli['income_z'] < -1.5, 'A',\n",
    "                     np.where(fmli['income_z'] < -.75, 'B',\n",
    "                     np.where(fmli['income_z'] < -.25, 'C',\n",
    "                     np.where(fmli['income_z'] < .25, 'D',\n",
    "                     np.where(fmli['income_z'] < .75, 'E', \n",
    "                     np.where(fmli['income_z'] < 1.5, 'F', 'G')))))),prefix=\"income\", dtype=int).reset_index(drop=True),\n",
    "    pd.get_dummies(\\\n",
    "                     np.where(fmli['fuel_z'] < -1.5, 'A',\n",
    "                     np.where(fmli['fuel_z'] < -.75, 'B',\n",
    "                     np.where(fmli['fuel_z'] < -.25, 'C',\n",
    "                     np.where(fmli['fuel_z'] < .25, 'D',\n",
    "                     np.where(fmli['fuel_z'] < .75, 'E', \n",
    "                     np.where(fmli['fuel_z'] < 1.5, 'F', 'G')))))),prefix=\"fuel\", dtype=int).reset_index(drop=True),\n",
    "    ], axis=1)\n",
    "\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291624a0-cb00-4a21-a660-d27a96e19434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedb7861-0d3d-45db-83dd-aaa2abd38926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe56886-4011-4c4a-bc20-1c103e894103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74cb9d2-b4cd-4a9d-b54f-9975eadda755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1a01c2e-3240-41fc-865e-33125a88ed91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TOTEXP for white\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.7}\n",
      "Best score:  0.38891555070877076\n",
      "    0.2705674171447754\n",
      "Duration: 222 seconds\n",
      "Fitting TOTEXP for black\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.2}\n",
      "Best score:  0.4498751759529114\n",
      "    0.34131187200546265\n",
      "Duration: 204 seconds\n",
      "Fitting TOTEXP for asian\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.2}\n",
      "Best score:  0.35883508920669555\n",
      "    0.2680951952934265\n",
      "Duration: 203 seconds\n",
      "Fitting TOTEXP for other\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.2}\n",
      "Best score:  0.22902863025665282\n",
      "    0.3356393575668335\n",
      "Duration: 204 seconds\n",
      "Fitting TOTEXP for hispanic\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.7}\n",
      "Best score:  0.40043174028396605\n",
      "    0.2464655637741089\n",
      "Duration: 205 seconds\n",
      "Fitting FOOD for white\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.7}\n",
      "Best score:  0.32032252792470134\n",
      "    0.2493057630758011\n",
      "Duration: 223 seconds\n",
      "Fitting FOOD for black\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.2}\n",
      "Best score:  0.24012939442885975\n",
      "    0.19042336878503008\n",
      "Duration: 205 seconds\n",
      "Fitting FOOD for asian\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.7}\n",
      "Best score:  0.10967432645485062\n",
      "    0.10745111228611859\n",
      "Duration: 204 seconds\n",
      "Fitting FOOD for other\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.2}\n",
      "Best score:  0.18879923258730139\n",
      "    0.14775446225222322\n",
      "Duration: 204 seconds\n",
      "Fitting FOOD for hispanic\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.2}\n",
      "Best score:  0.25267273234364623\n",
      "    0.189864100821573\n",
      "Duration: 206 seconds\n",
      "Fitting HOUS for white\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.7}\n",
      "Best score:  0.25799285672915595\n",
      "    0.27704048594641495\n",
      "Duration: 223 seconds\n",
      "Fitting HOUS for black\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.2}\n",
      "Best score:  0.2783739483816106\n",
      "    0.22079379619067285\n",
      "Duration: 205 seconds\n",
      "Fitting HOUS for asian\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 1, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.7}\n",
      "Best score:  0.2637078930517962\n",
      "    0.09748991844004984\n",
      "Duration: 204 seconds\n",
      "Fitting HOUS for other\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 1, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.7}\n",
      "Best score:  0.36715946164408325\n",
      "    0.28192718906266523\n",
      "Duration: 203 seconds\n",
      "Fitting HOUS for hispanic\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.2}\n",
      "Best score:  0.19751138463361934\n",
      "    0.19777997932240943\n",
      "Duration: 207 seconds\n",
      "Fitting TRANS for white\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.7}\n",
      "Best score:  0.047610267521428204\n",
      "    0.048258696327852735\n",
      "Duration: 222 seconds\n",
      "Fitting TRANS for black\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.2}\n",
      "Best score:  -0.03288105977595872\n",
      "    0.03134980627457684\n",
      "Duration: 204 seconds\n",
      "Fitting TRANS for asian\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.2}\n",
      "Best score:  -0.0135768697877513\n",
      "    0.01723621357683469\n",
      "Duration: 203 seconds\n",
      "Fitting TRANS for other\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.2}\n",
      "Best score:  -0.3042376964360749\n",
      "    0.04396597187022644\n",
      "Duration: 201 seconds\n",
      "Fitting TRANS for hispanic\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.2}\n",
      "Best score:  -0.02698906977533071\n",
      "    0.016102709747814803\n",
      "Duration: 193 seconds\n",
      "Fitting HEALTH for white\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.8, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.2}\n",
      "Best score:  0.03683372736661698\n",
      "    0.11065573139422935\n",
      "Duration: 208 seconds\n",
      "Fitting HEALTH for black\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 1, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.7}\n",
      "Best score:  0.28248281886682125\n",
      "    0.12963888436671112\n",
      "Duration: 195 seconds\n",
      "Fitting HEALTH for asian\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 1, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.7}\n",
      "Best score:  0.2517202970791927\n",
      "    0.08438618492006378\n",
      "Duration: 197 seconds\n",
      "Fitting HEALTH for other\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.2}\n",
      "Best score:  0.039636074355063354\n",
      "    0.09299332857663523\n",
      "Duration: 202 seconds\n",
      "Fitting HEALTH for hispanic\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.7}\n",
      "Best score:  0.19606973736201425\n",
      "    0.12982619050939204\n",
      "Duration: 205 seconds\n",
      "Fitting RETPEN for white\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.2}\n",
      "Best score:  0.6234166014811727\n",
      "    0.520105153182145\n",
      "Duration: 223 seconds\n",
      "Fitting RETPEN for black\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.8, 'eval_metric': 'rmse', 'lambda': 1, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.7}\n",
      "Best score:  0.7195593428120234\n",
      "    0.5842985475617326\n",
      "Duration: 203 seconds\n",
      "Fitting RETPEN for asian\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.7}\n",
      "Best score:  0.5800472555033107\n",
      "    0.5462562821664165\n",
      "Duration: 203 seconds\n",
      "Fitting RETPEN for other\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.7}\n",
      "Best score:  0.6060296248832683\n",
      "    0.4709327274052506\n",
      "Duration: 204 seconds\n",
      "Fitting RETPEN for hispanic\n",
      "Best set of hyperparameters:  {'colsample_bytree': 0.6, 'eval_metric': 'rmse', 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'objective': 'reg:squarederror', 'subsample': 0.7}\n",
      "Best score:  0.6922691556595293\n",
      "    0.5669730036991874\n",
      "Duration: 206 seconds\n",
      "Models saved\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Regression models\n",
    "#\n",
    "\n",
    "\n",
    "# The models will be kept in a dictionary of dictionaries\n",
    "races = ['white','black','asian','other','hispanic',]\n",
    "targets = ['TOTEXP','FOOD','HOUS','TRANS','HEALTH','RETPEN',]\n",
    "\n",
    "models = {}\n",
    "for target in targets:\n",
    "    targetmodels = {}\n",
    "    for race in races:\n",
    "        starting = datetime.now()\n",
    "        print(\"Fitting\",target, \"for\", race)\n",
    "        X = fmli[independents][fmli[\"model_\"+race] == 1]\n",
    "        y = fmli[target][fmli[\"model_\"+race] == 1]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test)        \n",
    "        model = xgb.XGBRegressor()\n",
    "        param_grid = {\n",
    "            'objective': ['reg:squarederror'],  \n",
    "            'max_depth': [3, 5], # [1, 3, 7],\n",
    "            'learning_rate': [0.01],  # also called eta.  Tried 0.75, 0.5, 0.25, 0.1, \n",
    "            'subsample': [0.2, 0.7], #, 0.2, 0.3, 0.4, 0.5, 0.7\n",
    "            'colsample_bytree':  [ 0.6, 0.8],  # 0.4, 0.6, \n",
    "            'lambda': [1, 2], # regularization 1, 2, 4\n",
    "            'n_estimators': [1000], # 100, 250, \n",
    "            'eval_metric':['rmse'],\n",
    "        }\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "    \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        print(\"Best set of hyperparameters: \", grid_search.best_params_)\n",
    "        print(\"Best score: \", grid_search.best_score_)\n",
    "        model = xgb.train(grid_search.best_params_, dtrain, num_boost_round=100)\n",
    "        y_pred = model.predict(dtest)\n",
    "        print(\"   \",r2_score(y_test, y_pred))\n",
    "        targetmodels[race] = model\n",
    "        duration = datetime.now() - starting\n",
    "        print(f\"Duration: {duration.seconds} seconds\")\n",
    "    models[target] = targetmodels\n",
    "\n",
    "with open(\"blockgroup_regression_models.pkl\", 'wb') as file:  \n",
    "    pickle.dump(models, file)\n",
    "\n",
    "print(\"Models saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd578a-56ef-4a01-945b-1c2da145d56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f85138-ce85-4ce4-a290-2fe1a36477da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d9b28-11ab-4f01-bada-edfeea79ce89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5075a12-4abe-4339-aa10-bced9dfe0bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93bebba-8dbf-40d6-8326-b17cedba1aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb69f8bc-9b08-412a-84a6-64aeefb1c90f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ccea8-7b16-4e15-8f9a-9ce67a391ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee92978-f3c6-4281-aad9-b4208e9c8bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blockgroupspending",
   "language": "python",
   "name": "blockgroupspending"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
